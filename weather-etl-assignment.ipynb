{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear Data Science Team,\n",
    "\n",
    "We are seeking your expertise to develop an ETL (Extract, Transform, Load) pipeline that will enable us to analyze historical weather data. Our focus is on understanding the changes in temperature and precipitation across the 48 contiguous United States between the years 1950 and 2000.\n",
    "\n",
    "Your primary data source will be the NOAA GSOD dataset, accessed here: https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516.  Please use the gzip bulk download source\n",
    "\n",
    "We need detailed statistical data on temperature and precipitation, aggregated monthly. This data should include metrics such as mean, median, variance, minimum, and maximum values.\n",
    "\n",
    "Technical Requirements:\n",
    "All scripts and data processing must be conducted in Jupyter Notebook. \n",
    "Please set up a dedicated project folder and Git repository for this assignment, using the naming convention <firstname>-<lastname>-weather-etl.  For example, Tom Cruise would name his repository -tom-cruise-weather-etl. All code should be done in a jupyter notebook \n",
    "\n",
    "Goals:\n",
    "1. Programatically (in python) download the appropriate gzip data from the link above one at a time.\n",
    "2. Unzip the data, and then delete the gzip file. \n",
    "3. Go through each csv and filter / clean out the appropriate data into a dataframe. This may require other libraries, such as reverse_geocoder.\n",
    "4. Delete the csvs when you are finished with them.\n",
    "5. Repeat steps 3-4 until you have a months worth of data, then transform that data to get the requested information above.\n",
    "6. Repeat steps 1-5 for each month and then for year between 1950 and 2000.\n",
    "At this point you should have a fully transformed dataset with yearly statistical data between 1950 and 2000.\n",
    "7. Export that data into a postgres database using sql alchemy.\n",
    "NOTE:  You will want to stop your existing container from running, then start a fresh database by making a new docker-compose file.  Ensure you have a .gitignore file so that the data on this postgres database isn't stored in git.\n",
    "\n",
    "Final Execution:\n",
    "Upon completion of the above steps, the following actions should replicate the database successfully:\n",
    "- Run docker-compose up\n",
    "- Run the jupyter notebook.\n",
    "\n",
    "End-State:\n",
    "- The project should be finalized with a clean workspace, meaning no unnecessary files remain in the project directory, and a fully operational database reflecting the processed data.\n",
    "\n",
    "Project Maintenance (KEEP THESE IN MIND):\n",
    "- Efficient Development: Be mindful of the time taken to download files. Develop strategies to test your program without needing to download all 50 years of data simultaneously.\n",
    "- File and Memory Management: Ensure the deletion of files post-processing and avoid retaining unnecessary data in memory. This approach is crucial to prevent system overload or program crashes due to excessive memory usage or storage constraints.\n",
    "- Many of these steps may require you to do additional research to complete.  Ensure you have understanding of the code in your project, even if its code copied from online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import tarfile\n",
    "from io import BytesIO\n",
    "import shutil\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_weather_file(start_year, end_year):\n",
    "    try:\n",
    "        base_url = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/\"\n",
    "        finalized_df = []\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            url = f\"{base_url}{year}.tar.gz\"\n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                    with tarfile.open(fileobj=BytesIO(response.content), mode=\"r:gz\") as tar:\n",
    "                        # Extract all members to a temporary directory\n",
    "                        tar.extractall(path=\"temp_dir\")\n",
    "                        \n",
    "                        # Combine CSV files into a single DataFrame\n",
    "                        df_list = []\n",
    "                        for member in tar.getmembers():\n",
    "                            if member.isfile() and (not member.name.startswith('71' or '76')) and member.name.endswith('.csv'):\n",
    "                                csv_content = tar.extractfile(member).read()\n",
    "                                df = pd.read_csv(BytesIO(csv_content))\n",
    "                                df_list.append(df)\n",
    "                        \n",
    "                        # Concatenate DataFrames\n",
    "                        combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "                        # lower-case columns\n",
    "                        combined_df.columns = combined_df.columns.str.lower()\n",
    "\n",
    "                        # filter out latitudes and longitudes\n",
    "                        combined_df = combined_df[\n",
    "                            (combined_df[\"latitude\"] > 25) & (combined_df[\"latitude\"] < 50) & \n",
    "                            (combined_df[\"longitude\"] > -125) & (combined_df[\"longitude\"] < -65)\n",
    "                            ]\n",
    "                        \n",
    "                        # replace 99.99 with null\n",
    "                        combined_df[\"temp\"].replace(99.99, np.nan, inplace=True)\n",
    "                        combined_df[\"prcp\"].replace(99.99, np.nan, inplace=True) \n",
    "                        combined_df[\"date\"] = pd.to_datetime(combined_df[\"date\"])\n",
    "\n",
    "                        # aggregating temps and prcps\n",
    "                        temp_monthly_stats = combined_df.groupby([combined_df[\"date\"].dt.to_period('M')]).agg({'temp': ['mean', 'median', 'var', 'min', 'max',]}).reset_index()\n",
    "                        prcp_monthly_stats = combined_df.groupby([combined_df[\"date\"].dt.to_period('M')]).agg({'prcp': ['mean', 'median', 'var', 'min', 'max',]}).reset_index()\n",
    "\n",
    "                        # set and reset index after aggregating temps and prcps\n",
    "                        temp_monthly_stats.set_index('date', inplace=True)\n",
    "                        prcp_monthly_stats.set_index('date', inplace=True)\n",
    "                        combined_df = pd.concat([temp_monthly_stats, prcp_monthly_stats], axis=1)\n",
    "                        combined_df = combined_df.reset_index()\n",
    "\n",
    "                        # convert columns \"date\" back to yyyy/mm/dd\n",
    "                        combined_df[\"date\"] = combined_df[\"date\"].dt.to_timestamp()\n",
    "\n",
    "                        # resetting column names\n",
    "                        new_column_names = [\n",
    "                            \"date\",\n",
    "                            \"temp_mean\", \"temp_median\", \"temp_var\", \"temp_min\", \"temp_max\",\n",
    "                            \"prcp_mean\", \"prcp_median\", \"prcp_var\", \"prcp_min\", \"prcp_max\"\n",
    "                        ]\n",
    "                        combined_df.columns = new_column_names\n",
    "\n",
    "                        # append to finalized dataframe\n",
    "                        finalized_df.append(combined_df)\n",
    "\n",
    "        # combine finalized dataframes\n",
    "        combined_finalized_df = pd.concat(finalized_df)\n",
    "\n",
    "        # exporting to postgres database\n",
    "        db_url = 'postgresql://postgres:password@localhost:5432/postgres'\n",
    "        engine = create_engine(db_url)\n",
    "        combined_finalized_df.to_sql('weather_stats_1950_2000', engine, index=False, if_exists='replace')   \n",
    "\n",
    "        return None\n",
    "\n",
    "                         \n",
    "                \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download or extract the file: {e}\")\n",
    "        return None\n",
    "    except tarfile.TarError as e:\n",
    "        print(f\"Failed to extract the file: {e}\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"No CSV files found in the archive.\")\n",
    "        return None\n",
    "    finally:\n",
    "        shutil.rmtree(\"temp_dir\", ignore_errors=True)\n",
    "        return print(\"Files extracted successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "# run this code to start extracting files\n",
    "weather_stats = extract_weather_file(1950, 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
